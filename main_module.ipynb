{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81572d3b",
   "metadata": {},
   "source": [
    "# Student Grade Prediction Models: Multiple Regression and Classification\n",
    "\n",
    "**Objective:** Prediction of the final grade of Portugese secondary education students using Regression and classification machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4304396",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07fda41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.output_wrapper .output {overflow-y: visible;height: fit-content;}</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a079e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries.\n",
    "import pandas as pd # Importing pandas\n",
    "import numpy as np # Importing numpy\n",
    "import seaborn as sns #Importing Seaborn for data visualisation\n",
    "import matplotlib.pyplot as plt #Importing matplotlib for data visualisation\n",
    "from sklearn.preprocessing import OrdinalEncoder,StandardScaler, LabelEncoder# Importing Ordinal encoder to encode the categorical data\n",
    "from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier# Importing random forest regression\n",
    "from sklearn.preprocessing import MinMaxScaler # Importing to do standardisation\n",
    "from sklearn.model_selection import train_test_split #Importing to split our data for training\n",
    "from sklearn.model_selection import cross_val_score,RandomizedSearchCV, GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import r2_score,confusion_matrix, classification_report,mean_squared_error\n",
    "from collections import Counter \n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "%matplotlib inline \n",
    "sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2eca21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eda_module import * #Importing the eda_module\n",
    "global studentDataSet\n",
    "studentDataSet = getDataSet()\n",
    "data = studentDataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ac4042",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae06294",
   "metadata": {},
   "source": [
    "# Data Mining "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69485f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the EDA function from EDA module\n",
    "dataInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1205a02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Statistics Details\n",
    "#Calling the function from EDA module\n",
    "statistics(studentDataSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaa821b",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62860b11",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Calling the function to show all the data visualization\n",
    "plotMain(studentDataSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4797d35",
   "metadata": {},
   "source": [
    "# Checking Collinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef070254",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the co-relation matrix\n",
    "checkCorrelation(studentDataSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c0375b",
   "metadata": {},
   "source": [
    "# Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d8069d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to divide the data into dependent& independent variable\n",
    "def divideDependentNIndependent(studentDataSet):\n",
    "    # Set Dependent and independent variables\n",
    "    explanatoryVariables = studentDataSet.drop(['G3'], axis=1) # features\n",
    "    target = studentDataSet['G3']# target feature\n",
    "    return explanatoryVariables,target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bea10b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to transform categorical to numbers\n",
    "def transformCategorical(explanatoryVariables):\n",
    "    df = explanatoryVariables\n",
    "    df= df[['school', 'sex', 'address','famsize','Pstatus','Mjob','Fjob','reason','guardian','schoolsup','famsup','paid',\n",
    "        'activities','nursery','higher','internet','romantic']]\n",
    "    X_train__ = df \n",
    "    #Using Ordinalencoder to encode the categorical variables\n",
    "    oe = OrdinalEncoder()\n",
    "    oe.fit(X_train__)\n",
    "    X_train_enc = oe.transform(X_train__).astype(int)\n",
    "   #Adding coulmns to make the numpyarray to dataframe\n",
    "    df = pd.DataFrame(X_train_enc, columns = ['school', 'sex', 'address','famsize','Pstatus','Mjob','Fjob','reason','guardian','schoolsup','famsup','paid',\n",
    "        'activities','nursery','higher','internet','romantic'])\n",
    "    #Removing the current categorical columns of the explonatory variable set\n",
    "    tempDataSet = explanatoryVariables.drop(['school', 'sex', 'address','famsize','Pstatus','Mjob','Fjob','reason','guardian','schoolsup','famsup','paid',\n",
    "        'activities','nursery','higher','internet','romantic'], axis = 1)    \n",
    "    # merging the new numerical data set with temporary dataset\n",
    "    dataSet= pd.merge(tempDataSet,df,  right_index=True, left_index=True)\n",
    "    #Reindex the column to make it as older index\n",
    "    column_names = ['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu',\n",
    "       'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime',\n",
    "       'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery',\n",
    "       'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc',\n",
    "       'Walc', 'health', 'absences', 'G1', 'G2']\n",
    "    dataSet = dataSet.reindex(columns=column_names)\n",
    "    return dataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24de1e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardisation\n",
    "def scalerFn(x_train,x_test):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test  = scaler.transform(x_test)   \n",
    "    return x_train,x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc36a701",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spliting train and test set for machine learning\n",
    "def trainNtest(X,target):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,target,train_size=0.77)# The dataset is divided into 70 and 30\n",
    "    x_train,x_test = scalerFn(X_train, X_test)\n",
    "    return x_train,x_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1569b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to find the cross validation score\n",
    "def crossValidationScore(model,x,y):\n",
    "    scores = cross_val_score(model, x, y, cv=5)\n",
    "    print(\"Accuracy cross validation :\",round(scores.mean()*100,2),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a5d796",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to do hypertuning using gridSearchCV\n",
    "def tuningUsinggridSearchCv(model_name,param_grid,X_train,y_train):\n",
    "    gridsearch = GridSearchCV(model_name, param_grid, cv = 3, verbose=0, n_jobs = -1)\n",
    "    model = gridsearch.fit(X_train,y_train)\n",
    "    return model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c64943",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to do hypertuning using randomSearchCV\n",
    "def tuningUsingRandomSearchCv(model_name,param_grid,X_train,y_train):\n",
    "    random = RandomizedSearchCV(estimator = model_name, param_distributions = param_grid, cv = 3, verbose=0, n_jobs = -1)\n",
    "    model = random.fit(X_train,y_train)\n",
    "    return model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97271cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to evaluate the model\n",
    "def evaluate(model, test_features, test_labels):\n",
    "    predictions = model.predict(test_features)\n",
    "    errors = abs(predictions - test_labels)\n",
    "    modelAccuracy = model.score(X_test, y_test)\n",
    "    print('Model Performance')\n",
    "    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "    print(\"Accuracy ({}): {:.2f}%\".format(text, modelAccuracy*100))  \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfffed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the accuracy of the model\n",
    "def evaluate_model(model, class_balance,X_test, y_test):\n",
    "    modelAccuracy = model.score(X_test, y_test)\n",
    "    print(\"Accuracy ({}): {:.2f}%\".format(class_balance, modelAccuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483d0352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_model(model,class_balance,X_test,y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    c_matrix =confusion_matrix(y_test,y_pred)\n",
    "    clr = classification_report(y_test,y_pred)\n",
    "    plt.figure(figsize=(8,8))\n",
    "    sns.heatmap(c_matrix,annot=True, fmt='g', vmin=0, cbar=False, cmap='Blues')\n",
    "    plt.xlabel(\"Predicted Value\")\n",
    "    plt.ylabel(\"Actual Value\")\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.show()\n",
    "    print(clr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a146eaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the feature importance graph\n",
    "def plot_feature_importance(importance,names,model_type):\n",
    "\n",
    "    #Create arrays from feature importance and feature names\n",
    "    feature_importance = np.array(importance)\n",
    "    feature_names = np.array(names)\n",
    "\n",
    "    #Create a DataFrame using a Dictionary\n",
    "    data={'feature_names':feature_names,'feature_importance':feature_importance}\n",
    "    fi_df = pd.DataFrame(data)\n",
    "\n",
    "    #Sort the DataFrame in order decreasing feature importance\n",
    "    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n",
    "\n",
    "    #Define size of bar plot\n",
    "    plt.figure(figsize=(15,8))\n",
    "    #Plot Searborn bar chart\n",
    "    sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])\n",
    "    #Add chart labels\n",
    "    plt.title(model_type + 'FEATURE IMPORTANCE')\n",
    "    plt.xlabel('FEATURE IMPORTANCE')\n",
    "    plt.ylabel('FEATURE NAMES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7986bc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to categorise the target variable\n",
    "def categoriseTarget(data):\n",
    "    #classification â€“ fail is less than 9\n",
    "    cat = (0, 9, 14, 21)\n",
    "    cat_name = ['Fail','Good','High']\n",
    "    data['G3']= pd.cut(data['G3'], bins= cat, labels= cat_name,include_lowest=True)\n",
    "    student_grade_class = LabelEncoder()\n",
    "    data['G3']=student_grade_class .fit_transform(data['G3'])\n",
    "    return data['G3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87595186",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to check class imbalance by plotting the target\n",
    "def checkingImbalance(target):    \n",
    "    b=sns.countplot(x=target, \n",
    "                       facecolor=(0, 0, 0, 0),\n",
    "                       linewidth=5,\n",
    "                       edgecolor=sns.color_palette(\"dark\", 3))\n",
    "    plt.xlabel('Y-train',fontsize=15)\n",
    "    plt.ylabel('Count',fontsize=15)\n",
    "    plt.title('Y-train distribution plot')\n",
    "    x_labels = ['Fail','Good','High']\n",
    "    b.set_xticklabels(x_labels,fontsize=12)\n",
    "# checkingImbalance(targetVariable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3d22ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to create sample using SMOTE\n",
    "def applySmote(X_train,y_train):\n",
    "#     using Counter to display results of naive oversampling\n",
    "    x, y = SMOTE().fit_resample(X_train, y_train) \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c108de57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to create sample using Random undersampler\n",
    "def applyRandomSampler(x,y):\n",
    "    ros = RandomUnderSampler(random_state=0)\n",
    "    x, y = ros.fit_resample(x, y)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe03cb6",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfbc6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is divided to explanatory and target variables\n",
    "explanatoryVariables,target = divideDependentNIndependent(studentDataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa084bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking first 5 view observations of explanatoryVariables\n",
    "explanatoryVariables.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324d0d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking first 5 view observations of target variables\n",
    "target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b31387a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the categorical explanatory variables to numerical\n",
    "x = transformCategorical(explanatoryVariables)\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03590615",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e862fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUnction of randomforest model\n",
    "def randomForestRegressionModel(x,y):\n",
    "    X_train,X_test,y_train,y_test = trainNtest(x,y)#After scaling and spliting\n",
    "    rf = RandomForestRegressor()\n",
    "    random_grid = {\n",
    "                   'max_depth': [10, 20, 30, 50, 60, 70, 80,90,100, None],\n",
    "                   'min_samples_leaf': [1, 2, 4],\n",
    "                   'min_samples_split': [2, 5, 10],\n",
    "                   'n_estimators': [100,130,150, 180, 200],\n",
    "                   'random_state':[0,15,42]\n",
    "                    }\n",
    "    rf_random = tuningUsingRandomSearchCv(rf, random_grid,X_train,y_train)\n",
    "    # Fit the random search model\n",
    "    model = rf_random.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print('R2 value  : ',round(r2_score(y_test, y_pred),2))\n",
    "    Adj_r2 = 1 - (1-r2_score(y_test, y_pred)) * (len(y)-1)/(len(y)-x.shape[1]-1)\n",
    "    print('Adjusted r2 : ', round(Adj_r2,2))\n",
    "    y_true = y_test\n",
    "    y_pred = y_pred\n",
    "    modelAccuracy = model.score(X_test, y_test)\n",
    "    print('Mean Squared Error:', round(mean_squared_error(y_test, y_pred),2))\n",
    "    print('Mean Absolute Error:', round(metrics.mean_absolute_error(y_test, y_pred),2))\n",
    "    print('Root Mean Squared Error:', round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),2))\n",
    "    print('Accuracy :',round(modelAccuracy,2))\n",
    "    x_axis = y_pred\n",
    "    y_axis = y_test - y_pred\n",
    "    sns.residplot (x=y_pred, y=(y_test - y_pred), lowess = True)\n",
    "    plt.title( 'Residual Plot' )\n",
    "    plt.xlabel('Predicted G3')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.ylabel ( 'Residuals')\n",
    "    # get importance\n",
    "    plot_feature_importance(model.feature_importances_,explanatoryVariables.columns,'RANDOM FOREST ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea384dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the model\n",
    "y_pred = randomForestRegressionModel(x,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f24033",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b495c4c",
   "metadata": {},
   "source": [
    "Let's categorise the G3 variable into 3 different categories based on the grade awarded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23b9623",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the first 5 entries of x to confirm the values are changed to numerical\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa13df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#View first 5 entries\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21758e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Y is categorising to specific group\n",
    "y = categoriseTarget(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86c95b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02167868",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = trainNtest(x,y)#Function to split train and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eda49d9",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952877f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM model creation\n",
    "from sklearn.svm import SVC\n",
    "def svmModel(x,y):\n",
    "    # SVM Model\n",
    "    svc = SVC(kernel='rbf', C=1, gamma='auto')\n",
    "    param_grid={'gamma':[1,1.5,2,3.5],'C': [1, 10], 'kernel': ('linear', 'rbf')}\n",
    "    model = tuningUsinggridSearchCv(svc,param_grid,X_train,y_train)# function used to tune the parameters to find out the best model\n",
    "    svc_model = model.fit(x,y)\n",
    "    return svc_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36187d31",
   "metadata": {},
   "source": [
    "# # Random Forest Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b231f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForestClassifier Model\n",
    "from  sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, truncnorm, randint\n",
    "def randomForestClassifier(X_train,y_train):\n",
    "    #From Grid search view to tune the model\n",
    "    rd = RandomForestClassifier()\n",
    "    param_grid= {'n_estimators': [100,113,150,200],'max_features': ['auto', 'sqrt', 'log2']}\n",
    "    model=tuningUsinggridSearchCv(rd,param_grid,X_train,y_train)\n",
    "#     rd = RandomForestClassifier(max_features='log2')\n",
    "    model = model.fit(X_train,y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee79f66f",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feafdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing MLPClassifier\n",
    "def multiLayerPerceptionNeuralNetworksModel(X_train,y_train):\n",
    "    #Initializing the MLPClassifier\n",
    "    #Tuned by using GridSearchcv\n",
    "    mpn = MLPClassifier()\n",
    "    param = {'hidden_layer_sizes':[150,100,50], 'max_iter': [800],'random_state':[1]}\n",
    "    mlpmodel = tuningUsinggridSearchCv(mpn,param,X_train, y_train)\n",
    "    model = mlpmodel.fit(X_train, y_train)#Predicting y for X_val\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cc1083",
   "metadata": {},
   "source": [
    "# Comparing the models with class imbalanced Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94020f8f",
   "metadata": {},
   "source": [
    "Evaluating the Svm model for imbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f1a97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the svm model for the imbalanced data\n",
    "model = svmModel(X_train,y_train)\n",
    "evaluate_model(model,\"Imbalanced Data:\", X_test,y_test)\n",
    "prediction_model(model,\"Imbalanced Data:\",X_test,y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bba15e",
   "metadata": {},
   "source": [
    "# Random forest Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7461032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the svm model for the imbalanced data\n",
    "model = randomForestClassifier(X_train,y_train)\n",
    "evaluate_model(model,\"Imbalanced Data:\", X_test,y_test)\n",
    "prediction_model(model,\"Imbalanced Data:\",X_test,y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6ed4d4",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d14a68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the svm model for the imbalanced data\n",
    "model = multiLayerPerceptionNeuralNetworksModel(X_train,y_train)\n",
    "evaluate_model(model,\"Imbalanced Data:\", X_test,y_test)\n",
    "prediction_model(model,\"Imbalanced Data:\",X_test,y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac023f0",
   "metadata": {},
   "source": [
    "# Checking the class Imbalance problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ade9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkingImbalance(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85089ca3",
   "metadata": {},
   "source": [
    "# SMOTE Oversampler to solve class imbalance issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9297976d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Applying smote to solve class imbalance issue\n",
    "x_samp,y_samp =  applySmote(X_train,y_train)\n",
    "checkingImbalance(y_samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a406c2",
   "metadata": {},
   "source": [
    "# Checking the model with sampled data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c3aa03",
   "metadata": {},
   "source": [
    "# SVM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d23512",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the svm model for the imbalanced data\n",
    "model = svmModel(x_samp,y_samp)\n",
    "evaluate_model(model,\"After sampling using SMOTE\", X_test,y_test)\n",
    "prediction_model(model,\"After sampling using SMOTE\",X_test,y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111000e0",
   "metadata": {},
   "source": [
    "# Random forest Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c809f461",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the svm model for the imbalanced data\n",
    "model = randomForestClassifier(x_samp,y_samp)\n",
    "evaluate_model(model,\"After sampling using SMOTE\", X_test,y_test)\n",
    "prediction_model(model,\"After sampling using SMOTE\",X_test,y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdee585",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a171ab8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the svm model for the imbalanced data\n",
    "model = multiLayerPerceptionNeuralNetworksModel(x_samp,y_samp)\n",
    "evaluate_model(model,\"After sampling using SMOTE\", X_test,y_test)\n",
    "prediction_model(model,\"After sampling using SMOTE\",X_test,y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3e25c6",
   "metadata": {},
   "source": [
    "# RandomUnderSampler to solve class imbalance issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3c9f9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Applying smote to solve class imbalance issue\n",
    "x_random,y_random =  applyRandomSampler(X_train,y_train)\n",
    "checkingImbalance(y_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a66ae30",
   "metadata": {},
   "source": [
    "# Checking the model on resampled data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6b99b8",
   "metadata": {},
   "source": [
    "# SVM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3885539",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the svm model for the imbalanced data\n",
    "model = svmModel(x_random,y_random)\n",
    "evaluate_model(model,\"After sampling using Random undersampler\", X_test,y_test)\n",
    "prediction_model(model,\"After sampling using Random undersampler\",X_test,y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722836ca",
   "metadata": {},
   "source": [
    "# Random Forest Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cbff59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the svm model for the imbalanced data\n",
    "model = randomForestClassifier(x_random,y_random)\n",
    "evaluate_model(model,\"After sampling using Random undersampler\", X_test,y_test)\n",
    "prediction_model(model,\"After sampling using Random undersampler\",X_test,y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef53298",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07215425",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the svm model for the imbalanced data\n",
    "model = multiLayerPerceptionNeuralNetworksModel(x_random,y_random)\n",
    "evaluate_model(model,\"After sampling using Random undersampler\", X_test,y_test)\n",
    "prediction_model(model,\"After sampling using Random undersampler\",X_test,y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab709035",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
